"""
DAG Worker : Scraping d'entreprises BCE
Ce DAG est triggered N fois en parall√®le par l'orchestrateur
"""

from airflow.decorators import dag, task
import pendulum
import time
import requests
from hdfs import InsecureClient
from bce_utils.proxy import ProxyService

REDIS_HOST = "redis"
REDIS_PORT = 6379
NAMENODE_URL = "http://namenode_bce:9870"
BCE_BASE_URL = "https://kbopub.economie.fgov.be"

@dag(
    dag_id='dag_exemple_scrappring',
    start_date=pendulum.datetime(2024, 1, 1, tz="UTC"),
    schedule=None,  # Triggered par l'orchestrateur
    catchup=False,
    tags=['exemple', 'worker', 'scrappring']
    # Note: max_active_runs se configure dans airflow.cfg ou docker-compose.yaml
)
def dag_exemple_scrappring():

    @task()
    def scrape_entreprise(**context):
        """Scrape une entreprise en utilisant le proxy assign√©"""
        # R√©cup√©rer les param√®tres depuis la configuration du DAG run
        dag_run_conf = context.get('dag_run').conf
        
        proxy_recu = dag_run_conf.get('proxy')
        entity_number = dag_run_conf.get('entity_number')
        denomination = dag_run_conf.get('denomination', 'N/A')
        
        print(f"üè¢ Scraping entreprise: {entity_number} - {denomination}")
        print(f"üåê Proxy assign√©: {proxy_recu}")
        
        # Cr√©er une nouvelle instance pour cette t√¢che
        proxy_service = ProxyService(redis_host=REDIS_HOST, redis_port=REDIS_PORT)
        client = InsecureClient(NAMENODE_URL, user="root")

        # Nombre r√©duit de tentatives car on utilise un seul proxy
        max_retries = 10
        
        # V√©rifier qu'on a bien re√ßu un proxy
        if not proxy_recu:
            print("‚ùå Aucun proxy assign√©")
            return {
                "status": "failed",
                "reason": "Aucun proxy assign√©",
                "entity_number": entity_number,
            }

        print(f"üîÑ Max tentatives: {max_retries} (avec le proxy assign√© uniquement)")
        
        for attempt in range(max_retries):
            try:
                print(f"üîÑ Tentative {attempt + 1}/{max_retries}")
                print(f"üåê Utilisation du proxy: {proxy_recu}")

                # Construire l'URL
                transformed_entity_number = entity_number.replace(".", "")
                url = f"{BCE_BASE_URL}/kbopub/zoeknummerform.html?lang=fr&nummer={transformed_entity_number}"

                # Faire la requ√™te avec le proxy assign√©
                proxies_dict = proxy_service.get_proxy_dict(proxy_recu)
                print(f"üåê URL: {url}")
                response = requests.get(
                    url,
                    proxies=proxies_dict,
                    timeout=15,
                    headers={"User-Agent": "Mozilla/5.0"},
                )
                print(f"üì° Status code: {response.status_code}")
                
                # G√©rer 404 - Ne pas stocker, mais ce n'est pas une erreur du proxy
                if response.status_code == 404:
                    print(f"‚ö†Ô∏è Entreprise {entity_number} non trouv√©e (404)")
                    return {
                        "status": "skipped",
                        "reason": "404",
                        "entity_number": entity_number,
                    }

                response.raise_for_status()
                html_content = response.text
                print(f"üìÑ HTML r√©cup√©r√©: {len(html_content)} caract√®res")

                response.raise_for_status()
                html_content = response.text

                # Sauvegarder dans HDFS
                hdfs_path = f"/hdfs-html-page/{entity_number}.html"
                with client.write(hdfs_path, overwrite=True) as writer:
                    writer.write(html_content.encode("utf-8"))

                print(f"‚úÖ Entreprise {entity_number} scrap√©e avec succ√®s")

                return {
                    "status": "success",
                    "entity_number": entity_number,
                    "proxy_used": proxy_recu,
                }

            except requests.exceptions.RequestException as e:
                print(f"‚ùå Erreur requ√™te (tentative {attempt + 1}): {e}")

                # # Marquer le proxy comme d√©faillant
                # if proxy_recu:
                #     proxy_service.mark_failed(proxy_recu)

                if attempt == max_retries - 1:
                    return {
                        "status": "failed",
                        "reason": f"√âchec apr√®s {max_retries} tentatives: {str(e)}",
                        "entity_number": entity_number,
                    }

            except Exception as e:
                print(f"‚ùå Erreur inattendue: {e}")
                import traceback
                traceback.print_exc()
                return {
                    "status": "failed",
                    "reason": str(e),
                    "entity_number": entity_number,
                    "proxy_used": proxy_recu,
                }
                
        return {
            "status": "failed",
            "reason": "√âchec apr√®s toutes les tentatives",
            "entity_number": entity_number,
            "proxy_used": proxy_recu,
        }
    
    # Appeler la t√¢che
    scrape_entreprise()



# Instancier le DAG
dag_exemple_scrappring()