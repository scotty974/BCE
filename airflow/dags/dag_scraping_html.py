from datetime import datetime

import requests
from airflow.decorators import dag, task
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from bce_utils.proxy import ProxyService
from hdfs import InsecureClient

# CONFIGURATION
REDIS_HOST = "redis"
REDIS_PORT = 6379
NAMENODE_URL = "http://namenode_bce:9870"
BCE_BASE_URL = "https://kbopub.economie.fgov.be"


@dag(
    dag_id="dag_scraping_html",
    start_date=datetime(2023, 1, 1),
    schedule="0 6 * * *",
    catchup=False,
)
def dag_scraping_html():
    entreprises_test = [
        {"entity_number": "0200.362.408", "denomination": "Test Company 1"},
        {"entity_number": "0200.420.410", "denomination": "Test Company 2"},
    ]

    @task()
    def get_valid_proxies():
        """RÃ©cupÃ¨re et initialise les proxies dans Redis"""
        proxy_service = ProxyService(redis_host=REDIS_HOST, redis_port=REDIS_PORT)

        try:
            print("ðŸ” RÃ©cupÃ©ration des proxies...")
            all_proxies = proxy_service.recuperer_tous_proxies()

            if not all_proxies:
                raise ValueError("Aucun proxy rÃ©cupÃ©rÃ© depuis les sources")

            print(f"ðŸ“‹ {len(all_proxies)} proxies rÃ©cupÃ©rÃ©s")

            print("ðŸ§ª Validation des proxies...")
            valid_proxies = proxy_service.valider_proxies(all_proxies, max_workers=10)

            if not valid_proxies:
                raise ValueError("Aucun proxy valide trouvÃ©")

            print(f"âœ… {len(valid_proxies)} proxies valides")

            # Initialiser Redis avec les proxies valides
            proxy_service.initialiser_proxies(valid_proxies)

            # VÃ©rifier les stats
            stats = proxy_service.get_stats()
            print(
                f"ðŸ“Š Stats Redis - Total: {stats['total']}, Disponibles: {stats['available']}"
            )

            return {"total_proxies": len(valid_proxies), "stats": stats}

        except Exception as e:
            print(f"âŒ Erreur lors de l'initialisation des proxies: {e}")
            raise

    @task()
    def scrape_entreprise(entreprise, proxy_init_result):
        """Scrape une entreprise en utilisant les proxies de Redis"""
        # CrÃ©er une nouvelle instance pour cette tÃ¢che
        proxy_service = ProxyService(redis_host=REDIS_HOST, redis_port=REDIS_PORT)
        client = InsecureClient(NAMENODE_URL, user="root")

        entity_number = entreprise["entity_number"]
        max_retries = 30

        print(f"ðŸ¢ Scraping entreprise: {entity_number}")

        # Charger les proxies depuis Redis
        try:
            proxy_service.charger_proxies_depuis_redis()
            print(f"âœ… {len(proxy_service.proxies)} proxies chargÃ©s depuis Redis")
        except Exception as e:
            print(f"âŒ Erreur chargement proxies: {e}")
            return {
                "status": "failed",
                "reason": f"Erreur chargement proxies: {str(e)}",
                "entity_number": entity_number,
            }

        print(f"max_retries: {max_retries}")
        client.makedirs("/hdfs-html-page/")
        for attempt in range(max_retries):
            try:
                print(f"ðŸ”„ Tentative {attempt + 1}/{max_retries}")

                # Attendre un proxy disponible
                proxy = proxy_service.attendre_proxy_disponible(timeout=300)

                if proxy is None:
                    print("âŒ Aucun proxy disponible")
                    if attempt == max_retries - 1:
                        return {
                            "status": "failed",
                            "reason": "Aucun proxy disponible aprÃ¨s timeout",
                            "entity_number": entity_number,
                        }
                    continue

                print(f"ðŸŒ Utilisation du proxy: {proxy}")

                # Marquer le proxy comme utilisÃ©
                proxy_service.mark_used(proxy)

                # Construire l'URL
                transformed_entity_number = entity_number.replace(".", "")
                url = f"{BCE_BASE_URL}/kbopub/zoeknummerform.html?lang=fr&nummer={transformed_entity_number}"

                # Faire la requÃªte
                proxies_dict = proxy_service.get_proxy_dict(proxy)
                print(f"ðŸŒ URL: {url}")
                response = requests.get(
                    url,
                    proxies=proxies_dict,
                    timeout=15,
                    headers={"User-Agent": "Mozilla/5.0"},
                )
                print(f"ðŸŒ Status code: {response.status_code}")
                # GÃ©rer 404
                if response.status_code == 404:
                    print(f"âš ï¸ Entreprise {entity_number} non trouvÃ©e (404)")
                    return {
                        "status": "skipped",
                        "reason": "404",
                        "entity_number": entity_number,
                    }

                response.raise_for_status()
                html_content = response.text

                # Sauvegarder dans HDFS
                hdfs_path = f"/hdfs-html-page/{entity_number}.html"
                with client.write(hdfs_path, overwrite=True) as writer:
                    writer.write(html_content.encode("utf-8"))

                print(f"âœ… Entreprise {entity_number} scrapÃ©e avec succÃ¨s")

                return {
                    "status": "success",
                    "entity_number": entity_number,
                    "proxy_used": proxy,
                }

            except requests.exceptions.RequestException as e:
                print(f"âŒ Erreur requÃªte (tentative {attempt + 1}): {e}")

                # Marquer le proxy comme dÃ©faillant
                if proxy:
                    proxy_service.mark_failed(proxy)

                if attempt == max_retries - 1:
                    return {
                        "status": "failed",
                        "reason": f"Ã‰chec aprÃ¨s {max_retries} tentatives: {str(e)}",
                        "entity_number": entity_number,
                    }

            except Exception as e:
                print(f"âŒ Erreur inattendue: {e}")
                return {
                    "status": "failed",
                    "reason": str(e),
                    "entity_number": entity_number,
                }
        return {
            "status": "failed",
            "reason": "Ã‰chec aprÃ¨s toutes les tentatives",
            "entity_number": entity_number,
        }

    # DÃ©finir les dÃ©pendances entre tÃ¢ches
    proxy_init = get_valid_proxies()

    # scrape_entreprise dÃ©pend de get_valid_proxies
    scraping_tasks = scrape_entreprise.partial(proxy_init_result=proxy_init).expand(
        entreprise=entreprises_test
    )
    
    # DÃ©clencher le DAG d'extraction Neo4j aprÃ¨s le scraping
    trigger_neo4j = TriggerDagRunOperator(
        task_id='trigger_neo4j_dag',
        trigger_dag_id='dag_bce_to_neo4j',
        wait_for_completion=False,  # Ne pas attendre la fin du DAG dÃ©clenchÃ©
    )
    
    # Le trigger se lance aprÃ¨s toutes les tÃ¢ches de scraping
    scraping_tasks >> trigger_neo4j


dag_scraping_html()