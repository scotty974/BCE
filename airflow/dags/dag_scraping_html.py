from datetime import datetime

import requests
from airflow.decorators import dag, task
from bce_utils.proxy import ProxyService
from hdfs import InsecureClient

# CONFIGURATION
REDIS_HOST = "redis"
REDIS_PORT = 6379
NAMENODE_URL = "http://namenode_bce:9870"
BCE_BASE_URL = "https://kbopub.economie.fgov.be"


@dag(
    dag_id="dag_scraping_html",
    start_date=datetime(2023, 1, 1),
    schedule="0 6 * * *",
    catchup=False,
)
def dag_scraping_html():
    entreprises_test = [
        {"entity_number": "0200.362.408", "denomination": "Test Company 1"},
        {"entity_number": "0200.420.410", "denomination": "Test Company 2"},
    ]

    @task()
    def get_valid_proxies():
        """R√©cup√®re et initialise les proxies dans Redis"""
        proxy_service = ProxyService(redis_host=REDIS_HOST, redis_port=REDIS_PORT)

        try:
            print("üîç R√©cup√©ration des proxies...")
            all_proxies = proxy_service.recuperer_tous_proxies()

            if not all_proxies:
                raise ValueError("Aucun proxy r√©cup√©r√© depuis les sources")

            print(f"üìã {len(all_proxies)} proxies r√©cup√©r√©s")

            print("üß™ Validation des proxies...")
            valid_proxies = proxy_service.valider_proxies(all_proxies, max_workers=10)

            if not valid_proxies:
                raise ValueError("Aucun proxy valide trouv√©")

            print(f"‚úÖ {len(valid_proxies)} proxies valides")

            # Initialiser Redis avec les proxies valides
            proxy_service.initialiser_proxies(valid_proxies)

            # V√©rifier les stats
            stats = proxy_service.get_stats()
            print(
                f"üìä Stats Redis - Total: {stats['total']}, Disponibles: {stats['available']}"
            )

            return {"total_proxies": len(valid_proxies), "stats": stats}

        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des proxies: {e}")
            raise

    @task()
    def scrape_entreprise(entreprise, proxy_init_result):
        """Scrape une entreprise en utilisant les proxies de Redis"""
        # Cr√©er une nouvelle instance pour cette t√¢che
        proxy_service = ProxyService(redis_host=REDIS_HOST, redis_port=REDIS_PORT)
        client = InsecureClient(NAMENODE_URL, user="root")

        entity_number = entreprise["entity_number"]
        max_retries = 30

        print(f"üè¢ Scraping entreprise: {entity_number}")

        # Charger les proxies depuis Redis
        try:
            proxy_service.charger_proxies_depuis_redis()
            print(f"‚úÖ {len(proxy_service.proxies)} proxies charg√©s depuis Redis")
        except Exception as e:
            print(f"‚ùå Erreur chargement proxies: {e}")
            return {
                "status": "failed",
                "reason": f"Erreur chargement proxies: {str(e)}",
                "entity_number": entity_number,
            }

        print(f"max_retries: {max_retries}")
        client.makedirs("/hdfs-html-page/")
        for attempt in range(max_retries):
            try:
                print(f"üîÑ Tentative {attempt + 1}/{max_retries}")

                # Attendre un proxy disponible
                proxy = proxy_service.attendre_proxy_disponible(timeout=300)

                if proxy is None:
                    print("‚ùå Aucun proxy disponible")
                    if attempt == max_retries - 1:
                        return {
                            "status": "failed",
                            "reason": "Aucun proxy disponible apr√®s timeout",
                            "entity_number": entity_number,
                        }
                    continue

                print(f"üåê Utilisation du proxy: {proxy}")

                # Marquer le proxy comme utilis√©
                proxy_service.mark_used(proxy)

                # Construire l'URL
                transformed_entity_number = entity_number.replace(".", "")
                url = f"{BCE_BASE_URL}/kbopub/zoeknummerform.html?lang=fr&nummer={transformed_entity_number}"

                # Faire la requ√™te
                proxies_dict = proxy_service.get_proxy_dict(proxy)
                print(f"üåê URL: {url}")
                response = requests.get(
                    url,
                    proxies=proxies_dict,
                    timeout=15,
                    headers={"User-Agent": "Mozilla/5.0"},
                )
                print(f"üåê Status code: {response.status_code}")
                # G√©rer 404
                if response.status_code == 404:
                    print(f"‚ö†Ô∏è Entreprise {entity_number} non trouv√©e (404)")
                    return {
                        "status": "skipped",
                        "reason": "404",
                        "entity_number": entity_number,
                    }

                response.raise_for_status()
                html_content = response.text

                # Sauvegarder dans HDFS
                hdfs_path = f"/hdfs-html-page/{entity_number}.html"
                with client.write(hdfs_path, overwrite=True) as writer:
                    writer.write(html_content.encode("utf-8"))

                print(f"‚úÖ Entreprise {entity_number} scrap√©e avec succ√®s")

                return {
                    "status": "success",
                    "entity_number": entity_number,
                    "proxy_used": proxy,
                }

            except requests.exceptions.RequestException as e:
                print(f"‚ùå Erreur requ√™te (tentative {attempt + 1}): {e}")

                # Marquer le proxy comme d√©faillant
                if proxy:
                    proxy_service.mark_failed(proxy)

                if attempt == max_retries - 1:
                    return {
                        "status": "failed",
                        "reason": f"√âchec apr√®s {max_retries} tentatives: {str(e)}",
                        "entity_number": entity_number,
                    }

            except Exception as e:
                print(f"‚ùå Erreur inattendue: {e}")
                return {
                    "status": "failed",
                    "reason": str(e),
                    "entity_number": entity_number,
                }
        return {
            "status": "failed",
            "reason": "√âchec apr√®s toutes les tentatives",
            "entity_number": entity_number,
        }

    # D√©finir les d√©pendances entre t√¢ches
    proxy_init = get_valid_proxies()

    # scrape_entreprise d√©pend de get_valid_proxies
    scrape_entreprise.partial(proxy_init_result=proxy_init).expand(
        entreprise=entreprises_test
    )


dag_scraping_html()
